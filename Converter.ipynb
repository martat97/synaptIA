{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc453506",
   "metadata": {},
   "source": [
    "# Converter di Formati da Relazionali a Non-Relazionali\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0038b8",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e97480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from bson import ObjectId\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb5cfc",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bad0c",
   "metadata": {},
   "source": [
    "#### E' stato applicato per velocizzare l'algoritmo per fare sampling di alcuni elementi, ci si aspetterà nella struttura dati che tutti vengano salvati. Di seguito la definizione del metodo che lo fa, impostando una lunghezza massima di righe. N_sample è il numero righe che voglio prendere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26b6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(max_len, n_sample):\n",
    "    random.seed(46)\n",
    "    # Verifica che il numero di righe di cui fare sample non sia superiore al numero massimo effettivo di righe\n",
    "    actual_n_sample = n_sample\n",
    "    if (n_sample > max_len):\n",
    "        actual_n_sample = max_len  \n",
    "    row_list = []\n",
    "    for i in range(0,10):\n",
    "        if (i >= actual_n_sample):\n",
    "            break\n",
    "        row_list.append(i)\n",
    "    if ( actual_n_sample - 10 > 0):\n",
    "        row_list += random.sample(range(10, max_len), actual_n_sample-10)\n",
    "    #print(row_list)\n",
    "    \n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace16c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path, n_sample): \n",
    "    data = None\n",
    "    nrow = 0\n",
    "    root, extension = os.path.splitext(file_path)\n",
    "    new_sample = None\n",
    "    try:\n",
    "        if (extension == '.csv'):\n",
    "            data = pd.read_csv(file_path, encoding='latin1')\n",
    "            \n",
    "            \n",
    "            new_sample = sample(data, n_sample)\n",
    "            nrow = len(data)  \n",
    "            #conversione da effettuare SE json ha problemi con colonne timestamp  \n",
    "            #for col in new_sample.columns:\n",
    "            #    if (pd.api.types.is_datetime64_any_dtype(new_sample[col])):\n",
    "            #        new_sample[col] = new_sample[col].astype(str)\n",
    "        elif (extension == '.xlsx'):\n",
    "            data = pd.read_excel(file_path) \n",
    "            #conversione da effettuare perchè json ha problemi con colonne timestam                \n",
    "            new_sample = sample(data, n_sample)  \n",
    "            nrow = len(data)   \n",
    "            #for col in new_sample.columns:\n",
    "            #    if (pd.api.types.is_datetime64_any_dtype(new_sample[col])):\n",
    "            #        new_sample[col] = new_sample[col].astype(str)\n",
    "        #Prendo prime 30 colonne\n",
    "        new_sample = new_sample.iloc[:, :30]\n",
    "        #Gestione valori Nan nei dataset\n",
    "        #1. Colonne con la maggioranza di valori nan, li toglo\n",
    "        new_sample = new_sample.dropna(axis=1, thresh=len(new_sample) * 0.6)  # Rimuove colonne con più dell'60% di NaN\n",
    "        #2. Ora, tolgo eventuali righe che contengono nan (per qualsiasi colonna) e resetto gli indici\n",
    "        new_sample = new_sample.dropna().reset_index(drop=True) \n",
    "\n",
    "    except Exception as e: #se ha problemi a leggere file tabulari sbagliati\n",
    "        data = None\n",
    "        # Ignora l'eccezione e continua senza stampare l'errore\n",
    "        pass\n",
    "\n",
    "\n",
    "    return new_sample, nrow\n",
    "        \n",
    "def sample(data, n_sample):\n",
    "    row_sampled = random_sampling(len(data),n_sample)\n",
    "    sample_data = data.loc[row_sampled]\n",
    "    return sample_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d427d24",
   "metadata": {},
   "source": [
    "## Rilevazione Tipi di Variabili\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9859d9b",
   "metadata": {},
   "source": [
    "#### Rileva la tipologia di variabile classificandomela in: numerica, categorica (codificabile con stringhe o numeri in un insieme discreto), testuale, con date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68415c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_NUMERIC, 'it_IT.utf-8')  # Adatta la localizzazione al tuo ambiente\n",
    "\n",
    "#Check caso in cui ho numeri messi in stringhe\n",
    "def is_numeric_variable(values):\n",
    "    cvalues = str(values)\n",
    "    locale.setlocale(locale.LC_NUMERIC, 'it_IT.utf-8') \n",
    "\n",
    "    def is_numeric(value):\n",
    "        try:\n",
    "            standardized_value = locale.atof(value)\n",
    "            return isinstance(standardized_value, (int, float))\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    return all(map(is_numeric, cvalues))\n",
    "        \n",
    "#Check presenza date\n",
    "def is_valid_date(date_string):\n",
    "    date_formats = ['%Y-%m-%d', '%d-%m-%Y', '%m-%d-%Y', '%Y-%d-%m','%m-%Y','%Y-%m',\n",
    "                    '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y', '%Y/%d/%m', '%m/%Y', '%Y/%m',\n",
    "                   '%H:%M', '%H:%M:%S', '%m/%d/%Y %H:%M:%S']\n",
    "    valid = False\n",
    "    correct_format = None\n",
    "    for date in date_formats:\n",
    "        try:\n",
    "            # Prova a convertire la stringa in un oggetto datetime\n",
    "            datetime.strptime(date_string, date)\n",
    "            valid = True\n",
    "            correct_format = date\n",
    "            break\n",
    "        except ValueError:\n",
    "            valid = False\n",
    "            \n",
    "    #if (valid == False):\n",
    "        \n",
    "    return valid, correct_format\n",
    "\n",
    "#Rilevazione tipo variabile\n",
    "def variable_type_detector(variable):\n",
    "    vtype = \"\"\n",
    "    #è numerica (tipo int, o stringhe che rappresentano numeri)\n",
    "    if(pd.api.types.is_datetime64_any_dtype(variable)):\n",
    "        vtype = 'datetime'\n",
    "    elif(pd.api.types.is_numeric_dtype(variable) | is_numeric_variable(variable)):\n",
    "        #conta ripetizioni di valori\n",
    "        unique_counter = variable.nunique()\n",
    "        repetition_percentage = unique_counter/len(variable)\n",
    "        #if (unique_counter <= max_counter):\n",
    "        if (repetition_percentage < 0.7):\n",
    "            vtype = \"categorical numerical\"\n",
    "        else:\n",
    "            vtype = \"continuous numerical\" \n",
    "    #è testuale\n",
    "    else:\n",
    "        #conta ripetizioni di valori\n",
    "        unique_counter = variable.nunique()\n",
    "        repetition_percentage = unique_counter/len(variable)\n",
    "        #if (unique_counter <= max_counter):\n",
    "        if (repetition_percentage < 0.7):\n",
    "            vtype = \"categorical textual\"\n",
    "            #guardo il primo elemento, e applico uno split delle prime 3 parole (con lo spazio),\n",
    "            #verifico se almeno uno di questi è un data (alle volte ho data separata dall'ora)\n",
    "            first_element = str(variable[0])\n",
    "            for spl in first_element.split()[0:3]:\n",
    "                if (is_valid_date(spl)[0]):\n",
    "                    vtype = 'datetime'\n",
    "        else:\n",
    "            vtype = \"text\"\n",
    "            #guardo il primo elemento, e applico uno split delle prime 3 parole (con lo spazio),\n",
    "            #verifico se almeno uno di questi è un data (alle volte ho data separata dall'ora)\n",
    "            first_element = str(variable[0])\n",
    "            for spl in first_element.split()[0:3]:\n",
    "                if (is_valid_date(spl)[0]):\n",
    "                    vtype = 'datetime'\n",
    "            \n",
    "    return vtype\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01083ae",
   "metadata": {},
   "source": [
    "## Creazione Oggetto Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edbb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazone Json File\n",
    "def create_file_node(file_name, sample_data, nrow):\n",
    "    #default values\n",
    "    file_elements = file_name.split(\".\")\n",
    "    extension_type = \"undefined\"\n",
    "    features = []\n",
    "    n_features = 0\n",
    "    \n",
    "    if (file_elements[1] in ['csv', 'xlsx']):\n",
    "        extension_type = \"tabular\"\n",
    "        n_features = len(sample_data.columns)\n",
    "        #controlla se è time stamp la feature, la converte in stringa, altrimenti json non va \n",
    "        \n",
    "        features = [\n",
    "        {\n",
    "            \"feature_name\": str(col),\n",
    "            \"feature_datatype\": sample_data[col].dtype.name,\n",
    "            \"feature_type\": variable_type_detector(sample_data[col]),\n",
    "            \"elements_sampled\": sample_data[col].tolist()\n",
    "        }\n",
    "        for col in sample_data.columns         \n",
    "            \n",
    "    ]   \n",
    "    elif (file_elements[1] in ['jpg', 'png']):\n",
    "        extension_type = \"image\"   \n",
    "    \n",
    "    \n",
    "    new_file = {\n",
    "        \"file_name\": file_elements[0],\n",
    "        \"file_extension\": file_elements[1],\n",
    "        \"file_type\": extension_type,\n",
    "        \"n_istances\": nrow,\n",
    "        \"n_features\": n_features,\n",
    "        \"features_names\": sample_data.columns.tolist(),\n",
    "        \"features_content\": features\n",
    "    }\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ef87e",
   "metadata": {},
   "source": [
    "## Navigazione Cartelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0499a",
   "metadata": {},
   "source": [
    "#### Può capitare che mi ritrovo con un solo file tabulare, come cartelle multiple. Per ora, la struttura a cartelle viene eliminata, estraendo solo alcuni files come campione e mettendoli in un vettore (per ridurre i tempi). L'approccio può essere cambiato, sia mantenendo la struttra ad albero, sia cambiando quanti files o cartelle estrarre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdac234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametri da impostare per numero cartelle e numero files massimi da estrarre\n",
    "max_subfolders = 5\n",
    "max_files = 10\n",
    "\n",
    "def navigate_folders(root, subfolders_limit, files_limit, new_dataset_node, subfolder_count=0):\n",
    "#mi prende eventuali files, i primi 10\n",
    "    if os.path.isdir(root):\n",
    "        for current_root, dirs, files in os.walk(root):\n",
    "            for file in files[0:files_limit]:\n",
    "                file_path = os.path.join(current_root, file)\n",
    "                #Numero righe da estrarre\n",
    "                n_sample = 200\n",
    "                result_sample = extract(file_path, n_sample)\n",
    "                #print(result_sample)\n",
    "                file_sample = result_sample[0]\n",
    "                nrow = result_sample[1]\n",
    "                if file_sample is not None:\n",
    "                    #print(file_sample)\n",
    "                    new_file = create_file_node(file, file_sample, nrow)\n",
    "                    new_dataset_node[\"files\"].append(new_file)\n",
    "                    #print(file)\n",
    "\n",
    "            #se c'è una sottocartella, \"scava\" ad albero, limitando il numero di sottocartelle da esaminare\n",
    "            # Chiamata ricorsiva per le prime massime sottocartelle\n",
    "            if subfolder_count >= subfolders_limit:\n",
    "                break\n",
    "\n",
    "            for subfolder in dirs:\n",
    "                new_root = os.path.join(current_root, subfolder)\n",
    "                subfolder_count = subfolder_count + 1\n",
    "                navigate_folders(new_root, subfolders_limit, files_limit, new_dataset_node, subfolder_count)\n",
    "                # Verifica se abbiamo raggiunto il limite delle sottocartelle\n",
    "                if subfolder_count >= subfolders_limit:\n",
    "                    break\n",
    "    elif os.path.isfile(root):\n",
    "        #file_path = os.path.join(root, file)\n",
    "        n_sample = 200\n",
    "        result_sample = extract(root, n_sample)\n",
    "        #print(result_sample)\n",
    "        file_sample = result_sample[0]\n",
    "        nrow = result_sample[1]\n",
    "        if file_sample is not None:\n",
    "            new_file = create_file_node(os.path.basename(root), file_sample, nrow)\n",
    "            new_dataset_node[\"files\"].append(new_file)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d693ffa",
   "metadata": {},
   "source": [
    "## Main Training ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5ec18",
   "metadata": {},
   "source": [
    "#### Main() utilizzato come training con alcuni datasets scaricati da Kaggle, tutte le funzioni vengono messe insieme e viene già impostato un input e output di cartelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f470294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training():\n",
    "    #Directory\n",
    "    root_directory = 'kaggle_datasets'\n",
    "    output_root_directory = 'datapoints'\n",
    "    extensions = ['.csv', '.json', '.xlsx']\n",
    "    directories = ['classification', 'clustering', 'nlp', 'regression', 'time_series']\n",
    "    #files = [file for file in os.listdir(directory) if file.endswith(tuple(extensions))]\n",
    "\n",
    "    # Json final\n",
    "    datasets_json = {}\n",
    "    # Converte ciascun file in un JSON\n",
    "    for directory in directories:\n",
    "        category = directory \n",
    "        directory_path = root_directory + \"/\" + directory\n",
    "        #each folder is the name of the dataset\n",
    "        datasets = [dataset for dataset in os.listdir(directory_path)]\n",
    "        #in ciascuna cartella del dataset, posso aspettarmi di tutto, cartelle, files etc.. come faccio?\n",
    "        #possibile soluzione: prendo alpiù 5 cartelle e alpiù 10 files, nelle cartelle scavo ad albero in 1-2 livelli massimi,\n",
    "        #poi mi fermo e inserisco max 10 files nella struttura\n",
    "        for dataset in datasets:\n",
    "            dataset_path = root_directory + \"/\" + directory + \"/\" + dataset\n",
    "            new_dataset_node = {\"_id\": dataset, \"label\": category, \"files\": []}\n",
    "            navigate_folders(dataset_path, max_subfolders, max_files, new_dataset_node)\n",
    "        \n",
    "            json_data = json.dumps(new_dataset_node, indent=2, default=str)\n",
    "            json_path = output_root_directory  + \"/\" + directory + \"/\"\n",
    "            with open(json_path + dataset + \".json\", \"w\") as json_file:          \n",
    "                json_file.write(json_data)\n",
    "        print(category + \" done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c365a3",
   "metadata": {},
   "source": [
    "## Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fe56f",
   "metadata": {},
   "source": [
    "#### Il main() vero e proprio, eseguito prendendo in input una path contenente i datasets o cartelle di datasets e vengono convertiti in oggetti json e messi in una cartella output. Di default, la categoria del dataset non ce l'ho, posso impostare di rilevarla se presente (devo salvare il dato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca8cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(datasets_path, output_path):\n",
    "    #each folder is the name of the dataset\n",
    "    elements = [element for element in os.listdir(datasets_path)]\n",
    "    #in ciascuna cartella del dataset, posso aspettarmi di tutto, cartelle, files etc.. come faccio?\n",
    "    #possibile soluzione: prendo alpiù 5 cartelle e alpiù 10 files, nelle cartelle scavo ad albero in 1-2 livelli massimi,\n",
    "    #poi mi fermo e inserisco max 10 files nella struttura\n",
    "    for element in elements:\n",
    "        new_dataset_node = {\"_id\": os.path.splitext(element)[0], \"label\": 'classification', \"files\": []}\n",
    "        navigate_folders(datasets_path+'/'+element, 5, 10, new_dataset_node)\n",
    "        json_data = json.dumps(new_dataset_node, indent=2, default=str)\n",
    "        json_path = output_path\n",
    "        with open(json_path + '/' + os.path.splitext(element)[0] + \".json\", \"w\") as json_file:          \n",
    "            json_file.write(json_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d80d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('input_datasets', 'output_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75541fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf4715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e8bda7",
   "metadata": {},
   "source": [
    "### Sistemazioni Future\n",
    "#### - Gestire altre estensioni oltre che csv, excel?\n",
    "#### - Convertire subito se è numerica ma formato stringa..\n",
    "#### - json.dumps con default str, però magari la variabile timestamp da tenere il tipo di dato\n",
    "#### - le eccezioni come gestirle? per ora passo, potrei cambiare encoding mentre non riesce a fare read csv o excel\n",
    "#### - Possibile errore: nella rilevazione dei tipi di variabili, alle volte mette numerica categorica anche se non lo è con pochi sample, vedi bachelors-or-higher-degree-data-of-usa in time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd9bd00",
   "metadata": {},
   "source": [
    "### Altre note sui Formati di dataset possibili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONVERSIONE DI FORMATI ##\n",
    "\n",
    "#FASI\n",
    "#1. LETTURA estensione\n",
    "#2. TRADUZIONE in JSON\n",
    "\n",
    "\n",
    "#FORMATI DATASET (fonte: https://docs.italia.it/AgID/documenti-in-consultazione/lg-opendata-docs/it/bozza/allegato-b-standard-di-riferimento-e-formati-aperti.html)\n",
    "\n",
    "#!! PRINCIPALI !!\n",
    "##Formati aperti per i dati\n",
    "#CSV (Comma Separated Values)\n",
    "#JSON (JavaScript Object Notation)\n",
    "#XML (eXtensible Markup Language)\n",
    "#XLSX (Excel)\n",
    "#!!\n",
    "\n",
    "\n",
    "##Formati aperti più diffusi per i dati geografici\n",
    "#Shapefile\n",
    "#KML\n",
    "#GeoJSON\n",
    "#GML (Geography Markup Language)\n",
    "#GeoPackage\n",
    "\n",
    "##Formati aperti per i documenti\n",
    "#ODF (Open Document Format)\n",
    "#PDF\n",
    "#Akoma Ntoso\n",
    "\n",
    "##Formati per dati meteorologici\n",
    "#BUFR (Binary Universal Form for the Representation of meteorological data)\n",
    "#NetCDF (Network Common Data Form)\n",
    "#ASCII (American Standard Code for Information Interchange)\n",
    "#Avvisi Meteo: \n",
    "#CAP (Common Alerting Protocol), RSS (Really Simple Syndication)/Atom\n",
    "#Radar: \n",
    "#HDF5 (Hierarchical Data Format)\n",
    "#Modello NWP (Numerical weather prediction): \n",
    "#GRIB (General Representation of fields In Binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
