{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784fc9b3",
   "metadata": {},
   "source": [
    "### Cose da sistemare\n",
    "#### - Gestire altre estensioni oltre che csv, excel?\n",
    "#### - Convertire subito se è numerica ma formato stringa..\n",
    "#### - Solo 4 categorie per le variabili? Magari qualcosa che identifici data/tempo\n",
    "#### - Navigazione cartella con limiti come variabili globali\n",
    "#### - Sampling limite variabile globale\n",
    "#### - json.dumps con default str, però magari la variabile timestamp da tenere il tipo di dato\n",
    "#### - le eccezioni come gestirle? per ora passo, potrei cambiare encoding mentre non riesce a fare read csv o excel\n",
    "#### - nella rilevazione dei tipi di variabili, alle volte mette numerica categorica anche se non lo è con pochi sample, vedi bachelors-or-higher-degree-data-of-usa in time_series\n",
    "#### - is numeric variable con ([\"10,730.7\"]) dà false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STANDARDIZZATORE ##\n",
    "\n",
    "#FASI\n",
    "#1. LETTURA estensione\n",
    "#2. TRADUZIONE in JSON\n",
    "\n",
    "\n",
    "#FORMATI DATASET (fonte: https://docs.italia.it/AgID/documenti-in-consultazione/lg-opendata-docs/it/bozza/allegato-b-standard-di-riferimento-e-formati-aperti.html)\n",
    "\n",
    "#!! PRINCIPALI !!\n",
    "##Formati aperti per i dati\n",
    "#CSV (Comma Separated Values)\n",
    "#JSON (JavaScript Object Notation)\n",
    "#XML (eXtensible Markup Language)\n",
    "#XLSX (Excel)\n",
    "#!!\n",
    "\n",
    "\n",
    "##Formati aperti più diffusi per i dati geografici\n",
    "#Shapefile\n",
    "#KML\n",
    "#GeoJSON\n",
    "#GML (Geography Markup Language)\n",
    "#GeoPackage\n",
    "\n",
    "##Formati aperti per i documenti\n",
    "#ODF (Open Document Format)\n",
    "#PDF\n",
    "#Akoma Ntoso\n",
    "\n",
    "##Formati per dati meteorologici\n",
    "#BUFR (Binary Universal Form for the Representation of meteorological data)\n",
    "#NetCDF (Network Common Data Form)\n",
    "#ASCII (American Standard Code for Information Interchange)\n",
    "#Avvisi Meteo: \n",
    "#CAP (Common Alerting Protocol), RSS (Really Simple Syndication)/Atom\n",
    "#Radar: \n",
    "#HDF5 (Hierarchical Data Format)\n",
    "#Modello NWP (Numerical weather prediction): \n",
    "#GRIB (General Representation of fields In Binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2e97480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from bson import ObjectId\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c26b6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(max_len, n_sample):\n",
    "    random.seed(46)\n",
    "    # Verifica che il numero di righe di cui fare sample non sia superiore al numero massimo effettivo di righe\n",
    "    actual_n_sample = n_sample\n",
    "    if (n_sample > max_len):\n",
    "        actual_n_sample = max_len  \n",
    "    row_list = []\n",
    "    for i in range(0,10):\n",
    "        if (i >= actual_n_sample):\n",
    "            break\n",
    "        row_list.append(i)\n",
    "    if ( actual_n_sample - 10 > 0):\n",
    "        row_list += random.sample(range(10, max_len), actual_n_sample-10)\n",
    "    #print(row_list)\n",
    "    \n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ace16c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path, n_sample): \n",
    "    data = None\n",
    "    nrow = 0\n",
    "    root, extension = os.path.splitext(file_path)\n",
    "    new_sample = None\n",
    "    try:\n",
    "        if (extension == '.csv'):\n",
    "            data = pd.read_csv(file_path, encoding='latin1')\n",
    "            \n",
    "            #conversione da effettuare perchè json ha problemi con colonne timestamp  \n",
    "            new_sample = sample(data, n_sample)\n",
    "            nrow = len(data)  \n",
    "            #for col in new_sample.columns:\n",
    "            #    if (pd.api.types.is_datetime64_any_dtype(new_sample[col])):\n",
    "            #        new_sample[col] = new_sample[col].astype(str)\n",
    "        elif (extension == '.xlsx'):\n",
    "            data = pd.read_excel(file_path) \n",
    "            #conversione da effettuare perchè json ha problemi con colonne timestam                \n",
    "            new_sample = sample(data, n_sample)  \n",
    "            nrow = len(data)   \n",
    "            #for col in new_sample.columns:\n",
    "            #    if (pd.api.types.is_datetime64_any_dtype(new_sample[col])):\n",
    "            #        new_sample[col] = new_sample[col].astype(str)\n",
    "        #Prendo prime 30 colonne\n",
    "        new_sample = new_sample.iloc[:, :30]\n",
    "        #Gestione valori Nan nei dataset\n",
    "        #1. Colonne con la maggioranza di valori nan, li toglo\n",
    "        new_sample = new_sample.dropna(axis=1, thresh=len(new_sample) * 0.6)  # Rimuove colonne con più dell'60% di NaN\n",
    "        #2. Ora, tolgo eventuali righe che contengono nan (per qualsiasi colonna) e resetto gli indici\n",
    "        new_sample = new_sample.dropna().reset_index(drop=True) \n",
    "\n",
    "    except Exception as e: #se ha problemi a leggere file tabulari sbagliati\n",
    "        data = None\n",
    "        # Ignora l'eccezione e continua senza stampare l'errore\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "    return new_sample, nrow\n",
    "        \n",
    "def sample(data, n_sample):\n",
    "    row_sampled = random_sampling(len(data),n_sample)\n",
    "    sample_data = data.loc[row_sampled]\n",
    "    return sample_data\n",
    "    \n",
    "#def extract_json(file_path, n_sample):\n",
    "#    with open(file_path, 'r') as file:\n",
    "#        data = json.load(file)\n",
    "#    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68415c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mi verifica se la lista è numerica nonostante la presenza di stringhe\n",
    "def is_numeric_variable(lst):\n",
    "    try:\n",
    "        # Prova a convertire ogni elemento in float\n",
    "        float_values = [float(element) for element in lst]\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # Se la conversione in float genera un errore, la lista non è completamente numerica\n",
    "        return False\n",
    "\n",
    "#identifico il tipo di variabile fra:\n",
    "#numerica discreta\n",
    "#numerica continua\n",
    "#categorica (parole per le categorie) o testuale (solo testi diversi)?\n",
    "\n",
    "def is_valid_date(date_string):\n",
    "    date_formats = ['%Y-%m-%d', '%d-%m-%Y', '%m-%Y','%Y-%m','%Y/%m/%d', '%d/%m/%Y', '%m/%Y', '%Y/%m']\n",
    "    valid = False\n",
    "    correct_format = None\n",
    "    for date in date_formats:\n",
    "        try:\n",
    "            # Prova a convertire la stringa in un oggetto datetime\n",
    "            datetime.strptime(date_string, date)\n",
    "            valid = True\n",
    "            correct_format = date\n",
    "            break\n",
    "        except ValueError:\n",
    "            valid = False\n",
    "    return valid, correct_format\n",
    "\n",
    "def variable_type_detector(variable):\n",
    "    vtype = \"\"\n",
    "    #è numerica (tipo int, o stringhe che rappresentano numeri)\n",
    "    if(pd.api.types.is_numeric_dtype(variable) | is_numeric_variable(variable)):\n",
    "        #conta ripetizioni di valori\n",
    "        unique_counter = variable.nunique()\n",
    "        #al massimo 15 categorie (esempio)\n",
    "        max_counter = 15 \n",
    "        if (unique_counter <= max_counter):\n",
    "            vtype = \"categorical numerical\"\n",
    "        else:\n",
    "            vtype = \"continuous numerical\"\n",
    "    #è testuale\n",
    "    else:\n",
    "        #conta ripetizioni di valori\n",
    "        unique_counter = variable.nunique()\n",
    "        #al massimo 15 categorie (esempio)\n",
    "        max_counter = 15 \n",
    "        if (unique_counter <= max_counter):\n",
    "            vtype = \"categorical textual\"\n",
    "            #guardo il primo elemento, e applico uno split delle prime 3 parole (con lo spazio),\n",
    "            #verifico se almeno uno di questi è un data (alle volte ho data separata dall'ora)\n",
    "            first_element = str(variable[0])\n",
    "            for spl in first_element.split()[0:3]:\n",
    "                if (is_valid_date(spl)[0]):\n",
    "                    vtype = 'date'\n",
    "        else:\n",
    "            vtype = \"text\"\n",
    "            #guardo il primo elemento, e applico uno split delle prime 3 parole (con lo spazio),\n",
    "            #verifico se almeno uno di questi è un data (alle volte ho data separata dall'ora)\n",
    "            first_element = str(variable[0])\n",
    "            for spl in first_element.split()[0:3]:\n",
    "                if (is_valid_date(spl)[0]):\n",
    "                    vtype = 'date'\n",
    "            \n",
    "    return vtype\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4edbb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_node(file_name, sample_data, nrow):\n",
    "    #default values\n",
    "    file_elements = file_name.split(\".\")\n",
    "    extension_type = \"undefined\"\n",
    "    features = []\n",
    "    n_features = 0\n",
    "    \n",
    "    if (file_elements[1] in ['csv', 'xlsx']):\n",
    "        extension_type = \"tabular\"\n",
    "        n_features = len(sample_data.columns)\n",
    "        #controlla se è time stamp la feature, la converte in stringa, altrimenti json non va \n",
    "        \n",
    "        features = [\n",
    "        {\n",
    "            \"feature_name\": str(col),\n",
    "            \"feature_datatype\": sample_data[col].dtype.name,\n",
    "            \"feature_type\": variable_type_detector(sample_data[col]),\n",
    "            \"elements_sampled\": sample_data[col].tolist()\n",
    "        }\n",
    "        for col in sample_data.columns         \n",
    "            \n",
    "    ]   \n",
    "    elif (file_elements[1] in ['jpg', 'png']):\n",
    "        extension_type = \"image\"   \n",
    "    \n",
    "    \n",
    "    new_file = {\n",
    "        \"file_name\": file_elements[0],\n",
    "        \"file_extension\": file_elements[1],\n",
    "        \"file_type\": extension_type,\n",
    "        \"n_istances\": nrow,\n",
    "        \"n_features\": n_features,\n",
    "        \"features_names\": sample_data.columns.tolist(),\n",
    "        \"features_content\": features\n",
    "    }\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdac234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_folders(root, subfolders_limit, files_limit, new_dataset_node, subfolder_count=0):\n",
    "#mi prende eventuali files, i primi 10\n",
    "    if os.path.isdir(root):\n",
    "        for current_root, dirs, files in os.walk(root):\n",
    "            for file in files[0:files_limit]:\n",
    "                file_path = os.path.join(current_root, file)\n",
    "                n_sample = 100\n",
    "                result_sample = extract(file_path, n_sample)\n",
    "                #print(result_sample)\n",
    "                file_sample = result_sample[0]\n",
    "                nrow = result_sample[1]\n",
    "                if file_sample is not None:\n",
    "                    #print(file_sample)\n",
    "                    new_file = create_file_node(file, file_sample, nrow)\n",
    "                    new_dataset_node[\"files\"].append(new_file)\n",
    "                    #print(file)\n",
    "\n",
    "            #se c'è una sottocartella, \"scava\" ad albero, limitando il numero di sottocartelle da esaminare\n",
    "            # Chiamata ricorsiva per le prime massime sottocartelle\n",
    "            if subfolder_count >= subfolders_limit:\n",
    "                break\n",
    "\n",
    "            for subfolder in dirs:\n",
    "                new_root = os.path.join(current_root, subfolder)\n",
    "                subfolder_count = subfolder_count + 1\n",
    "                navigate_folders(new_root, subfolders_limit, files_limit, new_dataset_node, subfolder_count)\n",
    "                # Verifica se abbiamo raggiunto il limite delle sottocartelle\n",
    "                if subfolder_count >= subfolders_limit:\n",
    "                    break\n",
    "    elif os.path.isfile(root):\n",
    "        #file_path = os.path.join(root, file)\n",
    "        n_sample = 100\n",
    "        result_sample = extract(root, n_sample)\n",
    "        #print(result_sample)\n",
    "        file_sample = result_sample[0]\n",
    "        nrow = result_sample[1]\n",
    "        if file_sample is not None:\n",
    "            new_file = create_file_node(os.path.basename(root), file_sample, nrow)\n",
    "            new_dataset_node[\"files\"].append(new_file)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f470294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training():\n",
    "    #Directory\n",
    "    root_directory = 'kaggle_datasets'\n",
    "    output_root_directory = 'datapoints'\n",
    "    extensions = ['.csv', '.json', '.xlsx']\n",
    "    #directories = [dirc for dirc in os.listdir(root_directory)]\n",
    "    directories = ['classification', 'clustering', 'nlp', 'pca', 'regression', 'time_series']\n",
    "    #directories = ['time_series']\n",
    "    #files = [file for file in os.listdir(directory) if file.endswith(tuple(extensions))]\n",
    "\n",
    "    # Json final\n",
    "    datasets_json = {}\n",
    "    # Converte ciascun file in un JSON\n",
    "    for directory in directories:\n",
    "        category = directory \n",
    "        directory_path = root_directory + \"/\" + directory\n",
    "        #each folder is the name of the dataset\n",
    "        datasets = [dataset for dataset in os.listdir(directory_path)]\n",
    "        #in ciascuna cartella del dataset, posso aspettarmi di tutto, cartelle, files etc.. come faccio?\n",
    "        #possibile soluzione: prendo alpiù 5 cartelle e alpiù 10 files, nelle cartelle scavo ad albero in 1-2 livelli massimi,\n",
    "        #poi mi fermo e inserisco max 10 files nella struttura\n",
    "        for dataset in datasets:\n",
    "            dataset_path = root_directory + \"/\" + directory + \"/\" + dataset\n",
    "            new_dataset_node = {\"_id\": dataset, \"label\": category, \"files\": []}\n",
    "            navigate_folders(dataset_path, 5, 10, new_dataset_node)\n",
    "        \n",
    "            json_data = json.dumps(new_dataset_node, indent=2, default=str)\n",
    "            json_path = output_root_directory  + \"/\" + directory + \"/\"\n",
    "            with open(json_path + dataset + \".json\", \"w\") as json_file:          \n",
    "                json_file.write(json_data)\n",
    "        print(category + \" done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dca8cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(datasets_path, output_path):\n",
    "    #extensions = ['.csv', '.json', '.xlsx']  \n",
    "    #each folder is the name of the dataset\n",
    "    elements = [element for element in os.listdir(datasets_path)]\n",
    "    #in ciascuna cartella del dataset, posso aspettarmi di tutto, cartelle, files etc.. come faccio?\n",
    "    #possibile soluzione: prendo alpiù 5 cartelle e alpiù 10 files, nelle cartelle scavo ad albero in 1-2 livelli massimi,\n",
    "    #poi mi fermo e inserisco max 10 files nella struttura\n",
    "    for element in elements:\n",
    "        new_dataset_node = {\"_id\": os.path.splitext(element), \"label\": None, \"files\": []}\n",
    "        navigate_folders(datasets_path+'/'+element, 5, 10, new_dataset_node)\n",
    "        json_data = json.dumps(new_dataset_node, indent=2, default=str)\n",
    "        json_path = output_path\n",
    "        #print(os.path.splitext(element))\n",
    "        with open(json_path + '/' + os.path.splitext(element)[0] + \".json\", \"w\") as json_file:          \n",
    "            json_file.write(json_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00f0ff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_8660\\1853817128.py:8: DtypeWarning: Columns (26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification done\n",
      "clustering done\n",
      "nlp done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_8660\\1853817128.py:8: DtypeWarning: Columns (14,15,16,17,18,19,20,22,23,24,25,49,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, encoding='latin1')\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_8660\\1853817128.py:8: DtypeWarning: Columns (16,17,18,19,20,22,23,24,25,26,27,28,45,49,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, encoding='latin1')\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_8660\\1853817128.py:8: DtypeWarning: Columns (14,15,16,17,18,19,20,22,23,24,25,49,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, encoding='latin1')\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_8660\\1853817128.py:8: DtypeWarning: Columns (16,17,18,19,20,22,23,24,25,26,27,28,45,49,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca done\n",
      "regression done\n",
      "time_series done\n"
     ]
    }
   ],
   "source": [
    "main_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('input_datasets', 'output_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a47f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0459c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333 entries, 0 to 332\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   realtime_start  333 non-null    object \n",
      " 1   realtime_end    333 non-null    object \n",
      " 2   date            333 non-null    object \n",
      " 3   value           333 non-null    float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 10.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Carica il file CSV\n",
    "#file_path = 'kaggle_datasets/time_series/advance-retail-sales-time-series-collection/MARTSMPCSM44W72USS.csv'\n",
    "#df = pd.read_csv(file_path)\n",
    "\n",
    "# Ottieni i metadati\n",
    "#info = df.info()\n",
    "\n",
    "# Visualizza i metadati\n",
    "#print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1fb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b444277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
